import pandas as pd
import numpy as np
import os
import joblib # For saving general sklearn pipelines if not using atom.save for everything
from atom import ATOMClassifier



def train_and_save_final_model(
    winner_model_name,
    dataset_path, # Path to the original CSV dataset
    target_column,
    positive_class_label, # This will now be used
    custom_hyperparameter_spaces,
    models_dir="./models",
    n_cv_folds=5,
    n_optuna_trials=50,
    cv_metric='f1', # Metric for hyperparameter tuning in this 5-fold CV step
    random_seed=42
):
    """
    Performs Task 4:
    1. Finds optimal hyperparameters for the winner model using 5-fold CV on the entire dataset.
    2. Trains the final model instance on all data with these optimal hyperparameters.
    3. Saves the trained ATOM model branch (including its preprocessing pipeline).

    Args:
        winner_model_name (str): The name of the winning algorithm (e.g., "RF", "LGB").
        dataset_path (str): Path to the original CSV dataset file.
        target_column (str): Name of the target variable column.
        positive_class_label (any): The label representing the positive class. This will be
                                     passed to ATOM's pos_label.
        custom_hyperparameter_spaces (dict): Dictionary of custom hyperparameter spaces
                                             for Optuna, as generated by a function like
                                             get_custom_hyperparameter_spaces().
        models_dir (str, optional): Directory to save the final model. Defaults to "./models".
        n_cv_folds (int, optional): Number of folds for cross-validation to find best params. Defaults to 5.
        n_optuna_trials (int, optional): Number of Optuna trials for hyperparameter search. Defaults to 50.
        cv_metric (str, optional): Metric to optimize during hyperparameter search. Defaults to 'f1'.
        random_seed (int, optional): Random seed for reproducibility. Defaults to 42.

    Returns:
        tuple: (dict, str)
               - Dictionary of the best hyperparameters found for the winner model.
               - Path to the saved final model (.pkl file).
               Returns (None, None) if an error occurs.
    """
    print(f"--- Starting Task 4: Training Final Model for '{winner_model_name}' ---")

    # --- 1. Load and Prepare Data ---
    try:
        print(f"Loading dataset from: {dataset_path}")
        data = pd.read_csv(dataset_path)
        # Basic cleaning if necessary (example, can be expanded)
        if 'id' in data.columns:
            data = data.drop(columns=['id'])
        if 'Unnamed: 32' in data.columns:
            data = data.drop(columns=['Unnamed: 32'])
        print(f"Dataset shape: {data.shape}")
    except FileNotFoundError:
        print(f"Error: Dataset file not found at {dataset_path}")
        return None, None
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return None, None

    # Separate features and target (original, before encoding for ATOM)
    X_original = data.drop(columns=[target_column])
    y_original_str = data[target_column] # Keep original string labels for ATOM

    # --- 2. Hyperparameter Tuning and Final Model Training with ATOM ---
    # ATOM will handle target encoding internally using the 'pos_label' parameter.

    print(f"\nInitializing ATOMClassifier for {winner_model_name} on the full dataset...")
    try:
        atom = ATOMClassifier(
            X_original, y_original_str,
            pos_label=positive_class_label, # <<< MODIFIED: Explicitly set the positive class label for ATOM
            test_size=0, # Use all data for training the final model after CV for hparam tuning
            random_state=random_seed,
            n_jobs=-1
        )
        print(f"ATOM initialized. Positive class label set to: '{positive_class_label}'. ATOM will encode '{positive_class_label}' as 1.")

        # Apply ATOM's preprocessing (these steps become part of the saved ATOM pipeline)
        print("Applying ATOM's internal imputation (default strategy)...")
        atom.impute() # Default: mean for numerical, mode for categorical

        print("Applying ATOM's internal scaling (RobustScaler)...")
        atom.scale(strategy="robust") # Or "standard", "minmax"

        # Get the specific hyperparameter space for the winner model
        winner_ht_space = None
        if custom_hyperparameter_spaces and winner_model_name in custom_hyperparameter_spaces:
            winner_ht_space = {winner_model_name: custom_hyperparameter_spaces[winner_model_name]}
            print(f"Using custom hyperparameter space for {winner_model_name}.")
        else:
            print(f"Warning: Custom hyperparameter space not found for {winner_model_name}. ATOM will use default search space.")

        print(f"\nRunning {n_cv_folds}-fold CV for hyperparameter tuning of {winner_model_name}...")
        print(f"Optimization metric: {cv_metric}, Trials: {n_optuna_trials}")
        atom.run(
            models=winner_model_name, # Only the winner model
            metric=cv_metric,
            n_trials=n_optuna_trials,
            cv=n_cv_folds, # N-fold CV on the entire dataset
            ht_params=winner_ht_space,
            errors='raise'
        )

        # The model branch is now trained on the full dataset with the best hyperparameters
        final_model_branch = atom.branch(winner_model_name)
        best_hyperparameters = final_model_branch.best_params
        print(f"\nBest hyperparameters found for {winner_model_name}:")
        print(best_hyperparameters)

        # --- 3. Save the Final Model ---
        if not os.path.exists(models_dir):
            print(f"Creating directory for models: {models_dir}")
            os.makedirs(models_dir)

        model_filename = f"{winner_model_name}_final_model_atom.pkl"
        model_save_path = os.path.join(models_dir, model_filename)

        print(f"\nSaving final trained ATOM model branch for {winner_model_name} to: {model_save_path}")
        # Save the specific model branch from ATOM. This includes ATOM's preprocessing.
        final_model_branch.save(model_save_path)
        # atom.save(model_save_path, models=winner_model_name) # Alternative way to save specific model

        print(f"Model saved successfully to {model_save_path}")
        print("--- Task 4 Finished ---")
        return best_hyperparameters, model_save_path

    except Exception as e:
        print(f"An error occurred during Task 4 processing: {e}")
        import traceback
        traceback.print_exc()
        return None, None